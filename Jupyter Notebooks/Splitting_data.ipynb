{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03bb267f-3d83-44e9-b106-24a6c3d38980",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is to split the data into train and test sets, respectively.\n",
    "Author: Booy Faassen\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "labels = pd.read_csv(\"./data/labels.csv\", header = None, delim_whitespace=True, names = [\"image\", \"value\", \"type\"])\n",
    "\n",
    "#print(labels)\n",
    "#print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b28ce67c-cea9-49fb-b33a-4cae84498f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Make binary classification and assign 0.3 to 0.0 and assign 0.6 to 1.0\"\"\"\n",
    "\n",
    "def binarize_dataset(labels):\n",
    "    labels_binary = labels.copy(deep=True)\n",
    "    \n",
    "    for row in range(len(labels_binary)):\n",
    "        if labels_binary.iloc[row]['value'] == 0.3333333333333333:\n",
    "            labels_binary.at[row, 'value'] = 0.0\n",
    "        elif labels_binary.iloc[row]['value'] == 0.6666666666666666:\n",
    "            labels_binary.at[row, 'value'] = 1.0\n",
    "    \n",
    "    return labels_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e0eeb81-a253-4c02-9822-fd1d7c334cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Split based on mono and poly\"\"\"\n",
    "\n",
    "def split_mono_poly(labels):\n",
    "    mono_labels = pd.DataFrame(columns = [\"image\", \"value\", \"type\"])\n",
    "    poly_labels = pd.DataFrame(columns = [\"image\", \"value\", \"type\"])\n",
    "    \n",
    "    for row in range(len(labels)):\n",
    "        if labels.iloc[row]['type'] == 'mono':\n",
    "            mono_labels.loc[len(mono_labels.index)] = [labels.iloc[row]['image'],\n",
    "                                                       labels.iloc[row]['value'],\n",
    "                                                       labels.iloc[row]['type']]\n",
    "        elif labels.iloc[row]['type'] == 'poly':\n",
    "            poly_labels.loc[len(poly_labels.index)] = [labels.iloc[row]['image'],\n",
    "                                                       labels.iloc[row]['value'],\n",
    "                                                       labels.iloc[row]['type']]\n",
    "    return mono_labels, poly_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98bb1086-d2e6-4721-847f-c52796b0fea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" stratified sampling \"\"\"\n",
    "\n",
    "def stratified_sampling(labels, train_fraction = 0.75):\n",
    "    train_set = labels.groupby(['type', 'value'], group_keys=False).apply(lambda x: x.sample(frac=train_fraction))\n",
    "\n",
    "    \"\"\" create a pandas dataframe for the test_set \"\"\"\n",
    "\n",
    "    test_set = pd.DataFrame(columns = [\"image\", \"value\", \"type\"])\n",
    "    count = 0\n",
    "    for row in range(len(labels)):\n",
    "        image = str(labels.iloc[row]['image'])\n",
    "        if image not in train_set['image'].unique():\n",
    "            count +=1\n",
    "            test_set.loc[len(test_set.index)] = [labels.iloc[row]['image'],\n",
    "                                                labels.iloc[row]['value'],\n",
    "                                                labels.iloc[row]['type']]\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aeedabb-753b-4df6-95f8-26f4b69d3b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ensure train and test folders are empty\"\"\"\n",
    "\n",
    "def empty_train_and_test_folders(data_dir = './data'):\n",
    "    test_dir_1 = data_dir + '/test/1.0'\n",
    "    test_dir_0 = data_dir + '/test/0.0'\n",
    "    train_dir_1 = data_dir + '/train/1.0'\n",
    "    train_dir_0 = data_dir + '/train/0.0'\n",
    "    \n",
    "    if os.listdir(test_dir_1): # True if list is not empty\n",
    "        for f in os.listdir(test_dir_1):\n",
    "            os.remove(test_dir_1 + '/' + f)\n",
    "\n",
    "    if os.listdir(test_dir_0):\n",
    "        for f in os.listdir(test_dir_0):\n",
    "            os.remove(test_dir_0 + '/' + f)\n",
    "\n",
    "    if os.listdir(train_dir_1):\n",
    "        for f in os.listdir(train_dir_1):\n",
    "            os.remove(train_dir_1 + '/' + f)\n",
    "\n",
    "    if os.listdir(train_dir_0):\n",
    "        for f in os.listdir(train_dir_0):\n",
    "            os.remove(train_dir_0 + '/' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d9adabd-6f81-4b23-b106-78be0d195761",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" copy images to the respective folders only works for binarized dataset\"\"\"\n",
    "\n",
    "def move_images_to_folders(train_set, test_set, data_dir = './data', all_data_dir = './data/all_data'):\n",
    "    test_dir_1 = data_dir + '/test/1.0'\n",
    "    test_dir_0 = data_dir + '/test/0.0'\n",
    "    train_dir_1 = data_dir + '/train/1.0'\n",
    "    train_dir_0 = data_dir + '/train/0.0'\n",
    "\n",
    "    for line in range(len(train_set)):\n",
    "        source = all_data_dir + '/' + str(train_set.iloc[line]['image'])[7:]\n",
    "        if train_set.iloc[line]['value'] == 1.0:\n",
    "            shutil.copy(source, train_dir_1)\n",
    "        elif train_set.iloc[line]['value'] == 0.0:\n",
    "            shutil.copy(source, train_dir_0)\n",
    "\n",
    "    for line in range(len(test_set)):\n",
    "        source = all_data_dir + '/' + str(test_set.iloc[line]['image'])[7:]\n",
    "        if test_set.iloc[line]['value'] == 1.0:\n",
    "            shutil.copy(source, test_dir_1)\n",
    "        elif test_set.iloc[line]['value'] == 0.0:\n",
    "            shutil.copy(source, test_dir_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd22b458-8a5c-4938-b31d-7f926f79eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" normal sampling (not stratified) \"\"\"\n",
    "\n",
    "def normal_sampling(labels, train_fraction = 0.75):\n",
    "    train_set = labels.sample(frac=train_fraction)\n",
    "\n",
    "    \"\"\" create a pandas dataframe for the test_set \"\"\"\n",
    "\n",
    "    test_set = pd.DataFrame(columns = [\"image\", \"value\", \"type\"])\n",
    "    count = 0\n",
    "    for row in range(len(labels)):\n",
    "        image = str(labels.iloc[row]['image'])\n",
    "        if image not in train_set['image'].unique():\n",
    "            count +=1\n",
    "            test_set.loc[len(test_set.index)] = [labels.iloc[row]['image'],\n",
    "                                                labels.iloc[row]['value'],\n",
    "                                                labels.iloc[row]['type']]\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67909d72-93c2-49b4-9c6b-c7de21f0c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Delete automatically created checkpoints \"\"\"\n",
    "\n",
    "def remove_checkpoints(path = './data'):\n",
    "    try:\n",
    "        shutil.rmtree(path + '/train/0.0/.ipynb_checkpoints')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree(path + '/train/1.0/.ipynb_checkpoints')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree(path + '/test/0.0/.ipynb_checkpoints')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree(path + '/test/1.0/.ipynb_checkpoints')\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2087f26c-bf30-43ad-8b02-3163ba06401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Make binary classification and remove labels 0.3 and 0.6 from the dataset\"\"\"\n",
    "\n",
    "def remove_unconfident_labels(labels):\n",
    "    new_labels = labels.copy(deep=True)\n",
    "    new_labels.drop(new_labels.loc[new_labels['value'] == 0.3333333333333333].index, inplace=True)\n",
    "    new_labels.drop(new_labels.loc[new_labels['value'] == 0.6666666666666666].index, inplace=True)\n",
    "    \n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "255a77f5-fb77-458b-a75e-226b9a6dd5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Split 0.0 and 1.0 labels from 0.3 and 0.6 labels \"\"\"\n",
    "\n",
    "def split_confident_unconfident_labels(labels):\n",
    "    confident_labels = pd.DataFrame(columns = [\"image\", \"value\", \"type\"])\n",
    "    unconfident_labels = pd.DataFrame(columns = [\"image\", \"value\", \"type\"])\n",
    "    \n",
    "    for row in range(len(labels)):\n",
    "        value = labels.iloc[row]['value']\n",
    "        if value == 1.0 or value == 0.0:\n",
    "            confident_labels.loc[len(confident_labels.index)] = [labels.iloc[row]['image'],\n",
    "                                                       labels.iloc[row]['value'],\n",
    "                                                       labels.iloc[row]['type']]\n",
    "        elif value == 0.3333333333333333 or value == 0.6666666666666666:\n",
    "            unconfident_labels.loc[len(unconfident_labels.index)] = [labels.iloc[row]['image'],\n",
    "                                                       labels.iloc[row]['value'],\n",
    "                                                       labels.iloc[row]['type']]\n",
    "    return confident_labels, unconfident_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea256b6d-8b07-4446-b225-1a905177937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Easy function to choose training setup\n",
    "\n",
    "arguments:\n",
    "option: takes values 1-7 for the dataset, see below for descriptions.\n",
    "path: the path at which the 'labels.csv' can be found.\n",
    "\n",
    "Options:\n",
    "1. binarize data, stratified sampling by Type (mono and poly) and Value (1.0 and 0.0)\n",
    "2. binarize data, normal sampling (train=75%), only Type mono\n",
    "3. binarize data, normal sampling (train=75%), only Type poly\n",
    "4. Only Type mono, use 1.0 and 0.0 as training, 0.3 and 0.6 as testing\n",
    "5. Only Type poly, use 1.0 and 0.0 as training, 0.3 and 0.6 as testing\n",
    "6. Only labels 0.0 and 1.0 without including 0.3 and 0.6 labels, both Types poly and mono, stratified sampling\n",
    "7. Only labels 0.0 and 1.0 without including 0.3 and 0.6 labels, only Type mono\n",
    "8. Only labels 0.0 and 1.0 without including 0.3 and 0.6 labels, only Type poly\n",
    "\"\"\"\n",
    "\n",
    "def training_data_initialisation(option = 1, data_dir = './data'):\n",
    "    \"\"\" Checking for valid input \"\"\"\n",
    "    valid_input = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "    if option not in valid_input:\n",
    "        raise ValueError(\"Option must be set to a value 1-8 (1 default, being stratified sampling after dataset binarization)\")\n",
    "\n",
    "    \"\"\" Function logic \"\"\"\n",
    "    # Logic to perform regardless of the option chosen\n",
    "    labels = pd.read_csv(os.path.join(data_dir, 'labels.csv'), header = None, delim_whitespace=True, names = [\"image\", \"value\", \"type\"])\n",
    "    \n",
    "    if option == 1:\n",
    "        binary_labels = binarize_dataset(labels)\n",
    "        train_set, test_set = stratified_sampling(binary_labels, train_fraction=0.75)\n",
    "        \n",
    "    elif option == 2:\n",
    "        binary_labels = binarize_dataset(labels)\n",
    "        mono_labels, _ = split_mono_poly(binary_labels)\n",
    "        train_set, test_set = normal_sampling(mono_labels, train_fraction=0.75)\n",
    "        \n",
    "    elif option == 3:\n",
    "        binary_labels = binarize_dataset(labels)\n",
    "        _, poly_labels = split_mono_poly(binary_labels)\n",
    "        train_set, test_set = normal_sampling(poly_labels, train_fraction=0.75)\n",
    "        \n",
    "    elif option == 4:\n",
    "        mono_labels, _ = split_mono_poly(labels)\n",
    "        confident_labels, unconfident_labels = split_confident_unconfident_labels(mono_labels)\n",
    "        unconfident_labels = binarize_dataset(unconfident_labels)\n",
    "        train_set, test_set = confident_labels, unconfident_labels\n",
    "        \n",
    "    elif option == 5:\n",
    "        _, poly_labels = split_mono_poly(labels)\n",
    "        confident_labels, unconfident_labels = split_confident_unconfident_labels(poly_labels)\n",
    "        unconfident_labels = binarize_dataset(unconfident_labels)\n",
    "        train_set, test_set = confident_labels, unconfident_labels\n",
    "        \n",
    "    elif option == 6:\n",
    "        confident_labels = remove_unconfident_labels(labels)\n",
    "        train_set, test_set = stratified_sampling(confident_labels, train_fraction=0.75)\n",
    "        \n",
    "    elif option == 7:\n",
    "        confident_labels = remove_unconfident_labels(labels)\n",
    "        mono_labels, _ = split_mono_poly(confident_labels)\n",
    "        train_set, test_set = normal_sampling(mono_labels, train_fraction=0.75)\n",
    "        \n",
    "    elif option == 8:\n",
    "        confident_labels = remove_unconfident_labels(labels)\n",
    "        _, poly_labels = split_mono_poly(confident_labels)\n",
    "        train_set, test_set = normal_sampling(poly_labels, train_fraction=0.75)\n",
    "\n",
    "    # Logic regardless of what option is chosen\n",
    "    return train_set, test_set\n",
    "    \n",
    "    \"\"\" Checking\n",
    "    remove_checkpoints()\n",
    "    if len(train_set) == len(os.listdir('data/train/1.0')) + len(os.listdir('data/train/0.0')) and len(test_set) == len(os.listdir('data/test/1.0')) + len(os.listdir('data/test/0.0')):\n",
    "        print('OK')\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9b1f13-399c-4c48-aa64-65524e6ce939",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create folders and sub folders for each dataset \"\"\"\n",
    "\n",
    "def create_dataset_folders(parent_dir = './data'):\n",
    "    for i in range(1,9):\n",
    "        directory = 'dataset0' + str(i)\n",
    "        path = os.path.join(parent_dir, directory)\n",
    "        os.mkdir(path)\n",
    "        create_sub_folders(path)\n",
    "\n",
    "\n",
    "def create_sub_folders(parent_dir):\n",
    "    folders = {\n",
    "        'train': os.path.join(parent_dir, 'train'),\n",
    "        'test': os.path.join(parent_dir, 'test'),\n",
    "        'train1.0': os.path.join(parent_dir, 'train/1.0'),\n",
    "        'train0.0': os.path.join(parent_dir, 'train/0.0'),\n",
    "        'test1.0': os.path.join(parent_dir, 'test/1.0'),\n",
    "        'test0.0': os.path.join(parent_dir, 'test/0.0')\n",
    "    }\n",
    "\n",
    "    for i in range(len(folders)):\n",
    "        os.mkdir(list(folders.values())[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c85d5c1-3477-4544-81dd-553ffd40a53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" populate dataset folders with the datasets \"\"\"\n",
    "\n",
    "def create_datasets(parent_dir = './data'):\n",
    "    create_dataset_folders(parent_dir = parent_dir)\n",
    "    datasets = ['dataset01', 'dataset02', 'dataset03', 'dataset04', 'dataset05', 'dataset06', 'dataset07', 'dataset08']\n",
    "    for i in range(len(datasets)):\n",
    "        train_set, test_set = training_data_initialisation(option = i+1, data_dir = parent_dir)\n",
    "        path = os.path.join(parent_dir, datasets[i])\n",
    "        empty_train_and_test_folders(data_dir = path)\n",
    "        move_images_to_folders(train_set, test_set, data_dir = path, all_data_dir = parent_dir + '/all_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31a40894-1eca-43d8-afba-ac4a9dd6a74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Remove all datasets \"\"\"\n",
    "\n",
    "def remove_all_datasets(parent_dir = './data'):\n",
    "    datasets = ['dataset01', 'dataset02', 'dataset03', 'dataset04', 'dataset05', 'dataset06', 'dataset07', 'dataset08']\n",
    "    for i in range(len(datasets)):\n",
    "        path = os.path.join(parent_dir, datasets[i])\n",
    "        remove_checkpoints(path)\n",
    "        empty_train_and_test_folders(path)\n",
    "        shutil.rmtree(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2585358-9155-430b-a25e-e8d20cd2c950",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Count number of images in a folder of a given path \"\"\"\n",
    "\n",
    "def count_images_in_folder(root = './data/dataset01', verbose: bool = False):\n",
    "    train10 = '/train/1.0'\n",
    "    train00 = '/train/0.0'\n",
    "    test10 = '/test/1.0'\n",
    "    test00 = '/test/0.0'\n",
    "\n",
    "    num_train_1 = 0\n",
    "    num_train_0 = 0\n",
    "    num_test_1 = 0\n",
    "    num_test_0 = 0\n",
    "    for i in os.listdir(root + train10):\n",
    "        num_train_1 += 1\n",
    "    for i in os.listdir(root + train00):\n",
    "        num_train_0 += 1\n",
    "    for i in os.listdir(root + test10):\n",
    "        num_test_1 += 1\n",
    "    for i in os.listdir(root + test00):\n",
    "        num_test_0 += 1\n",
    "    \n",
    "    num_train = num_train_1 + num_train_0\n",
    "    num_test = num_test_1 + num_test_0\n",
    "    num_total = num_train + num_test\n",
    "    \n",
    "    if verbose == True:\n",
    "        print('train 1.0:', num_train_1)\n",
    "        print('train 0.0:', num_train_0, '\\n')\n",
    "        print('test 1.0:', num_test_1)\n",
    "        print('test 0.0:', num_test_0, '\\n')\n",
    "        print('total 1.0:', num_train_1 + num_test_1)\n",
    "        print('total 0.0:', num_train_0 + num_test_0)\n",
    "    \n",
    "    return num_train, num_test, num_total \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ca0aad1-9eb4-4316-976f-3779f180393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Main method \"\"\"\n",
    "\n",
    "create_datasets('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d9cd55b-9e6f-40ff-ab55-a0d4e9f62341",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Main method to remove dataset folders \"\"\"\n",
    "\n",
    "remove_all_datasets(parent_dir = './data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "660fb707-40a3-4828-830c-21ae60cb40b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1.0: 616\n",
      "train 0.0: 1353 \n",
      "\n",
      "test 1.0: 205\n",
      "test 0.0: 450 \n",
      "\n",
      "total 1.0: 821\n",
      "total 0.0: 1803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1969, 655, 2624)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_images_in_folder(root='./data/dataset01', verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
